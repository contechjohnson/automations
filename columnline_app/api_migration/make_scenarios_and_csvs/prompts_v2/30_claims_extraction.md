# Claims Extraction

**Stage:** UTILITY
**Step:** CLAIMS_EXTRACTION
**Produces Claims:** TRUE
**Context Pack:** FALSE
**Model:** gpt-4.1

---

## Input Variables

**research_narrative**
Narrative text output from research step (entity research, lead enrichment, opportunity research, client-specific research, insight)

**step_name**
Which step produced this narrative (for claim tagging)

**extraction_rules** *(optional)*
Custom extraction rules from client's claims_extraction_prompt config

---

## Main Prompt Template

### Role
You are a claims extraction specialist converting research narratives into atomic, grounded, source-cited facts (claims).

### Objective
Parse research narrative and extract discrete claims: atomic facts grounded in sources, each with confidence level, source citation, and claim type. Claims become the structured data that flows through pipeline and populates final dossier.

### What You Receive
- Research narrative (long-form text from entity research, opportunity research, etc.)
- Step name (entity_research, enrich_lead, enrich_opportunity, client_specific, insight)
- Optional custom extraction rules from client config

### Instructions

**Phase 1: Understand Claim Types**

**1.1 Claim Type by Step**

| Step | Claim Types Produced |
|------|---------------------|
| **entity_research** | ENTITY, SIGNAL, OPPORTUNITY (initial) |
| **enrich_lead** | LEAD_PROFILE, NETWORK |
| **enrich_opportunity** | OPPORTUNITY (detailed) |
| **client_specific** | CLIENT_SPECIFIC |
| **insight** | INSIGHT |

**1.2 Claim Structure Requirements**

Every claim must have:
- **claim_id**: Unique identifier (step_name + sequence number)
- **claim_type**: ENTITY, SIGNAL, OPPORTUNITY, CONTACT, LEAD_PROFILE, CLIENT_SPECIFIC, INSIGHT
- **statement**: Atomic fact (one fact per claim, not compound)
- **entities**: Array of entities mentioned (company names, people names)
- **date_in_claim**: Date mentioned in claim (if applicable)
- **source_url**: URL to source material
- **source_name**: Publication or website name
- **source_tier**: GOV, PRIMARY, NEWS, OTHER
- **confidence**: HIGH, MEDIUM, LOW

**Phase 2: Extraction Process**

**2.1 Read Narrative**

Scan research narrative for facts:
- Company identity facts
- Project details
- Signal information
- Timeline milestones
- Competitive intelligence
- Relationship information
- Strategic insights

**2.2 Create Atomic Claims**

For each fact found:

**Good (Atomic):**
- "[Geography] removed Environmental Assessment requirement for [Project Name] access road on July 4, 2025"

**Bad (Compound):**
- "[Geography] removed EA requirement and approved the road corridor, which will enable construction to start in Q2 2026"

Split compound facts into separate claims:
1. "[Geography] removed EA requirement for [Project Name] access road on July 4, 2025"
2. "EA removal enables construction start Q2 2026"

**2.3 Extract Source Citations**

From narrative, identify sources:
- URLs should be in narrative (if not, extract from [Sources] section)
- Assign source tier (GOV if .gov, PRIMARY if company website, NEWS if publication, OTHER)
- Note source name

**2.4 Assess Confidence**

**HIGH Confidence:**
- From tier 1-2 sources (government, company press release)
- Multiple sources confirm same fact
- Recent information (< 6 months old)

**MEDIUM Confidence:**
- From tier 3 sources (news articles, trade publications)
- Single source, but credible
- Moderately recent (6-12 months)

**LOW Confidence:**
- From tier 4 sources (blogs, forums, social media)
- Inference or interpretation
- Older information (> 12 months)
- Conflicting information from sources

**2.5 Tag Claim Type**

Based on what the claim describes:

**ENTITY:**
- Company identity (name, ownership, location, size)
- Business model or operations
- Leadership (CEO, executives)
- Financial info (revenue, funding)

**SIGNAL:**
- Events indicating buying readiness (EPCM award, permit, approval, groundbreaking)
- Each signal = separate claim

**OPPORTUNITY:**
- Project specifications (scope, budget, timeline)
- Procurement status (contracts awarded, contracts open)
- Vendor opportunities
- Technical requirements

**CONTACT:**
- Individual decision-makers (name, title, role, relevance)
- Each contact = separate claim (or multiple claims per contact)

**LEAD_PROFILE:**
- Company strategic moves
- Competitive landscape
- How they buy
- Growth trajectory

**CLIENT_SPECIFIC:**
- Warm introduction paths
- Alumni networks
- Insider knowledge
- Relationship intelligence

**INSIGHT:**
- Competitive analysis
- Win probability factors
- Objection anticipation
- Positioning strategy

**Phase 3: Format Claims**

**3.1 Create Claim Objects**

For each extracted fact:

```json
{
  "claim_id": "entity_001",
  "claim_type": "ENTITY",
  "statement": "[Company Name] acquired [Project Name] project from [Company] Resources in 2022 for $435M",
  "entities": ["[Company Name]", "[Company] Resources"],
  "date_in_claim": "2022",
  "source_url": "https://wyloocanada.com/news/acquisition-2022",
  "source_name": "[Company Name] Press Release",
  "source_tier": "PRIMARY",
  "confidence": "HIGH"
}
```

**3.2 Apply Custom Extraction Rules**

If extraction_rules provided in client config:
- Follow client-specific extraction guidance
- Apply custom confidence thresholds
- Include/exclude certain claim types
- Use custom tagging or categorization

**Phase 4: Validate Claims**

**4.1 Quality Checks**

- Is each claim atomic (one fact)?
- Does each claim have source URL?
- Is confidence level justified?
- Are claim types correctly assigned?
- Are entities extracted correctly?

**4.2 Completeness Checks**

- Did we extract all major facts from narrative?
- Are signal claims separate (not bundled)?
- Are contact claims individual (not grouped)?
- Did we preserve source diversity?

### Output Format

Return valid JSON array of claims:

```json
{
  "claims": [
    {
      "claim_id": "entity_001",
      "claim_type": "ENTITY",
      "statement": "[Company Name] acquired [Project Name] project from [Company] Resources in 2022 for $435M",
      "entities": ["[Company Name]", "[Company] Resources"],
      "date_in_claim": "2022",
      "source_url": "https://wyloocanada.com/news/acquisition-2022",
      "source_name": "[Company Name] Press Release",
      "source_tier": "PRIMARY",
      "confidence": "HIGH"
    },
    {
      "claim_id": "signal_001",
      "claim_type": "SIGNAL",
      "statement": "[Geography] removed Environmental Assessment requirement for [Project Name] access road on July 4, 2025",
      "entities": ["[Geography]", "[Project Name]"],
      "date_in_claim": "2025-07-04",
      "source_url": "https://ero.ontario.ca/notice/019-8827",
      "source_name": "Environmental Registry of [Geography]",
      "source_tier": "GOV",
      "confidence": "HIGH"
    },
    {
      "claim_id": "signal_002",
      "claim_type": "SIGNAL",
      "statement": "[Partner Firm Name] awarded EPCM contract for [Project Name] mine on July 15, 2025",
      "entities": ["[Partner Firm Name]", "[Project Name]"],
      "date_in_claim": "2025-07-15",
      "source_url": "https://wyloocanada.com/news/epcm-award-2025",
      "source_name": "[Company Name] Press Release",
      "source_tier": "PRIMARY",
      "confidence": "HIGH"
    },
    {
      "claim_id": "opportunity_001",
      "claim_type": "OPPORTUNITY",
      "statement": "[Project Name] project total cost estimated at $1.2B",
      "entities": ["[Project Name]"],
      "date_in_claim": null,
      "source_url": "https://[industry from ICP].com/eagles-nest-cost-2025",
      "source_name": "Mining.com",
      "source_tier": "NEWS",
      "confidence": "MEDIUM"
    }
  ],
  "claims_count": 47,
  "claim_types_breakdown": {
    "ENTITY": 12,
    "SIGNAL": 8,
    "OPPORTUNITY": 15,
    "CONTACT": 7,
    "LEAD_PROFILE": 5
  },
  "source_tiers_breakdown": {
    "GOV": 3,
    "PRIMARY": 18,
    "NEWS": 22,
    "OTHER": 4
  },
  "confidence_breakdown": {
    "HIGH": 28,
    "MEDIUM": 15,
    "LOW": 4
  },
  "extraction_metadata": {
    "step_name": "entity_research",
    "narrative_length_tokens": 2400,
    "extraction_timestamp": "2026-01-12T10:25:00Z",
    "custom_rules_applied": false
  }
}
```

### Constraints

**Do:**
- Extract every significant fact from narrative
- Make claims atomic (one fact per claim)
- Include source URL for every claim
- Assess confidence realistically
- Tag claim types correctly

**Do NOT:**
- Create compound claims (split into atomic facts)
- Fabricate sources (if narrative lacks URL, mark confidence LOW)
- Over-extract (don't create claims for every sentence, just facts)
- Lose important information (better to extract too much than miss critical facts)
- Mis-assign claim types (SIGNAL is not ENTITY, CONTACT is not OPPORTUNITY)

**Claim Atomicity Examples:**

**Bad (Compound):**
"[Company Name] acquired [Project Name] in 2022 for $435M and hired [Contact Name] as COO to oversee project execution"

**Good (Atomic - 2 claims):**
1. "[Company Name] acquired [Project Name] from [Company] Resources in 2022 for $435M"
2. "[Contact Name] hired as COO at [Company Name] to oversee [Project Name] execution"

**Source Citation:**
- Every claim MUST have source_url (if narrative doesn't provide, flag for review)
- Source tier: GOV > PRIMARY > NEWS > OTHER
- Confidence should reflect source quality

**Claim Types:**
- ENTITY: Company facts
- SIGNAL: Buying readiness events
- OPPORTUNITY: Project details
- CONTACT: Decision-maker info
- LEAD_PROFILE: Strategic/competitive intel
- CLIENT_SPECIFIC: Relationship intelligence
- INSIGHT: Win probability, positioning, objections

---

## Variables Produced

- `claims` - Array of claim objects
- `claims_count` - Total claims extracted
- `claim_types_breakdown` - Count by type
- `source_tiers_breakdown` - Source quality distribution
- `confidence_breakdown` - Confidence distribution

---

## Integration Notes

**Model:** gpt-4.1 (sync, 1-2 min)

**Purpose:** Transform research narratives into structured claims. Claims are the atomic data units that flow through pipeline.

**Flow:**
1. Research step produces narrative (long-form text)
2. Claims Extraction parses narrative â†’ claims array
3. Claims flow to Claims Merge (consolidation + deduplication)
4. Merged claims flow to downstream steps and section writers

**Why Claims Matter:**
- **Traceability**: Every fact has source
- **Grounded**: Not hallucinations, backed by sources
- **Structured**: Easier to query, filter, and route to sections
- **Flexible**: Can be merged, filtered, validated
- **Audit trail**: Full research lineage preserved

**Custom Rules:**
- Clients can provide custom extraction rules in claims_extraction_prompt config
- Example: "Extract competitor mentions as separate claims"
- Example: "Flag any budget information with HIGH confidence threshold"
- Custom rules applied during extraction if provided
