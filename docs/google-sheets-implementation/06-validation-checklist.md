# Schema Validation Checklist

**Date:** 2026-01-12
**Status:** Pre-Import Validation

---

## âœ… Architecture Decisions Confirmed

### 1. Scenario Isolation âœ…
- [x] Pass IDs only (run_id, client_id, dossier_id)
- [x] Fetch configs/prompts within each scenario
- [x] Each scenario runnable independently
- [x] Main generates: run_id, dossier_id
- [x] Bridge scenarios pass: run_id, client_id, dossier_id, specific_object

### 2. Denormalization for Visibility âœ…
- [x] PipelineSteps has input (column F) and output (column G) columns
- [x] Full JSON in both columns (shows complete step execution)
- [x] Claims appear in both Claims sheet AND PipelineSteps.input/output
- [x] Accept duplication for debugging ease

### 3. API Call Count âœ…
- [x] Total per dossier: ~132 calls
- [x] Rate limit: 100 calls/100 seconds
- [x] Average: 0.2-0.4 calls/second (5-10 minute run)
- [x] Well within limits

### 4. Future-Proof for Supabase âœ…
- [x] Same scenario structure
- [x] Just swap Google Sheets modules â†’ HTTP modules
- [x] No logic changes needed

---

## âœ… All 14 Sheets Validated

### Config Layer (3 sheets)

#### 1. Clients Sheet âœ…
- [x] 13 columns (A-M)
- [x] All 3 configs have compressed versions (D-I: icp, industry, research_context)
- [x] client_specific_research in column J
- [x] Example row populated
- [x] **Used by:** All scenarios (read at start)

#### 2. Prompts Sheet âœ…
- [x] 12 columns (A-L)
- [x] 20 prompts populated (PRM_001 to PRM_020)
- [x] variables_used (column I) and variables_produced (column J) present
- [x] produce_claims flag (column G)
- [x] context_pack_produced flag (column H)
- [x] **Used by:** All scenarios (read at start to get prompt template)

#### 3. SectionDefinitions Sheet âœ…
- [x] 6 columns (A-F)
- [x] expected_variables, variable_types, validation_rules
- [x] 2 example sections (INTRO, SIGNALS)
- [x] **Used by:** Section validation (optional)

### Execution Layer (11 sheets)

#### 4. Onboarding Sheet âœ…
- [x] 15 columns (A-O)
- [x] Generates: icp_config, industry_research, research_context (originals)
- [x] Links to Clients via client_id (column M)
- [x] **Used by:** Onboarding process (Phase 0)

#### 5. PrepInputs Sheet âœ…
- [x] 13 columns (A-M)
- [x] Compresses all 3 configs (columns D-I)
- [x] token_savings column (K)
- [x] **Used by:** Config compression (Phase 0)

#### 6. BatchComposer Sheet âœ…
- [x] 10 columns (A-J)
- [x] batch_strategy contains distribution (no separate columns)
- [x] run_ids_created links to Runs (column H)
- [x] **Used by:** Batch planning (Phase 0)

#### 7. Runs Sheet âœ…
- [x] 10 columns (A-J)
- [x] run_id (A), client_id (B), status (C), dossier_id (E)
- [x] seed_data (D), config_snapshot (J)
- [x] NO output fields (lead_score, target_entity) - those go in Dossiers
- [x] **Used by:** Main pipeline (create + update)

#### 8. PipelineSteps Sheet âœ…
- [x] 13 columns (A-M)
- [x] **input column (F)** - Full JSON of what went into step âœ…
- [x] **output column (G)** - Full JSON of what came out âœ…
- [x] step_id (A), run_id (B), prompt_id (C), step_name (D)
- [x] tokens_used (I), runtime_seconds (J)
- [x] **Used by:** All scenarios (write after every LLM call)

#### 9. Claims Sheet âœ…
- [x] 5 columns (A-E)
- [x] run_id (A), step_id (B), step_name (C), claims_json (D)
- [x] Simple structure (one row per claims-producing step)
- [x] Full claims array in claims_json column (not parsed out)
- [x] **Used by:** All claims-producing steps (write), downstream steps (read)

#### 10. MergedClaims Sheet âœ…
- [x] 5 columns (A-E)
- [x] Decoupled from 07B_INSIGHT (that's just a regular step)
- [x] merged_claims_json (D) - same format as Claims.claims_json
- [x] Can have multiple rows per run (merge_id tracks)
- [x] **Used by:** Merge step (write), section writers (read)

#### 11. ContextPacks Sheet âœ…
- [x] 7 columns (A-G)
- [x] pack_content (D) - full context pack JSON
- [x] context_type (C) - e.g., signal_to_entity
- [x] consumed_by_steps (F) - tracks lineage
- [x] **Used by:** Context pack builder (write), downstream steps (read)

#### 12. Contacts Sheet âœ…
- [x] 32 columns (A-AG)
- [x] Columns A-S: Renderable (for app display)
- [x] Columns T-AF: Processing (for copy generation)
- [x] Both sets present (name, email, linkedin_url, bio_paragraph, etc.)
- [x] **Used by:** Contact enrichment (write), section writers (read)

#### 13. Sections Sheet âœ…
- [x] 9 columns (A-I)
- [x] section_data (D) - full section JSON
- [x] variables_produced (H) - tracks what fields were generated
- [x] target_column (I) - which JSONB column in Dossiers
- [x] **Used by:** Section writers (write), assembly (read)

#### 14. Dossiers Sheet âœ…
- [x] 12 columns (A-L)
- [x] 5 JSONB columns (H-L): find_lead, enrich_lead, copy, insight, media
- [x] Output fields here (lead_score, timing_urgency, company_name)
- [x] assembled_dossier (column not shown - might be separate or combined)
- [x] **Used by:** Final assembly (write)

---

## âœ… Data Flow Validated

### Main Pipeline Flow

```
START (API Call)
  â†“ generates: run_id, dossier_id
  â†“ receives: client_id, seed
  â†“
WRITE: Runs sheet (status="running")
  â†“
CALL SCENARIOS (pass: run_id, client_id, seed, dossier_id)
  â”œâ”€ Search & Signal â†’ Writes: PipelineSteps (2), Claims (1), ContextPacks (1)
  â”œâ”€ Enrich Lead â†’ Writes: PipelineSteps (1), Claims (1)
  â”œâ”€ Enrich Opportunity â†’ Writes: PipelineSteps (1), Claims (1)
  â”œâ”€ Client Specific â†’ Writes: PipelineSteps (1), Claims (1)
  â”œâ”€ Enrich Contacts (bridge)
  â”‚   â””â”€ Individual Contact Enrichment (Ã—10)
  â”‚       â””â”€ Writes: Contacts (1), PipelineSteps (1)
  â”œâ”€ Copy â†’ Writes: PipelineSteps (1), updates Contacts
  â”œâ”€ Insight â†’ Writes: PipelineSteps (1), Claims (1)
  â”œâ”€ Media â†’ Writes: PipelineSteps (1)
  â””â”€ Section Writers (bridge)
      â””â”€ Individual Section Writer (Ã—7)
          â””â”€ Writes: Sections (1), PipelineSteps (1)
  â†“
WRITE: Dossiers sheet (final assembly)
WRITE: Runs sheet (status="completed")
  â†“
END
```

**Total Writes:** 1 + 53 = 54 writes
**Total Reads:** 79 reads
**Total API Calls:** 133 calls âœ…

---

## âœ… Scenario Pattern Validated

### Standard Scenario Template

**Every scenario follows this pattern:**

```
[1] START (receive: run_id, client_id, seed, dossier_id)
  â†“
[2] READ: Clients sheet (filter by client_id)
  â†“ Get: icp_config_compressed, industry_research_compressed, etc.
  â†“
[3] READ: Prompts sheet (filter by slug)
  â†“ Get: prompt_template, model, variables_used
  â†“
[4] READ: Previous outputs (Claims/ContextPacks by run_id) - if needed
  â†“
[5] LLM CALL (use fetched prompt + config + previous outputs)
  â†“
[6] WRITE: PipelineSteps (columns F+G have full input/output JSON)
  â†“
[7] WRITE: Output sheet (Claims, Contacts, Sections, etc.) - if applicable
  â†“
END
```

**Validation:**
- [x] Can run in isolation (just needs IDs)
- [x] Fetches everything it needs
- [x] Writes complete execution log
- [x] No dependency on main pipeline having passed context

---

## âœ… Input/Output Columns Present

### PipelineSteps Sheet
- [x] Column F: **input** - Full JSON of what went into the LLM
- [x] Column G: **output** - Full JSON of what came back from LLM

**Example Row:**
```csv
STEP_RUN001_03, RUN_001, PRM_003, 3_ENTITY_RESEARCH, completed,
"{"candidate_entity": "Acme Corp", "signal_data": {...}, "context_pack": {...}}",
"{"entity_narrative": "Acme Corp is...", "confidence_assessment": "HIGH"}",
gpt-4.1, 1250, 12.5, 2026-01-12 14:30:22, 2026-01-12 14:30:25, ""
```

**What This Enables:**
- âœ… See exactly what went into step
- âœ… See exactly what came out
- âœ… Debug: "Why did this fail? Look at input column"
- âœ… Reconstruct: "What claims were used? Check input column"
- âœ… No joins needed - one row tells the story

---

## âœ… Missing Elements Check

### Things Intentionally NOT in Schema

- [x] No MASTER sheet (replaced with API parameters)
- [x] No rigid claim structure (kept as JSON blobs)
- [x] No separate columns for entity_claims, signal_claims (all in claims_json)
- [x] No google_doc_url in Dossiers (rendering in app instead)
- [x] No output fields in Runs sheet (lead_score, etc. in Dossiers)

### Things That Might Be Missing (But Okay)

- [ ] Claims extraction prompt (not in CSV, but defined in PIPELINE_PROMPTS list)
- [ ] Claims merge prompt (not in CSV, but defined in PIPELINE_PROMPTS list)
- [ ] Onboarding system prompt (might not exist as separate prompt)

**Status:** These can be added later if needed. Core 20 prompts are sufficient.

---

## âœ… Rate Limit Safety Check

**Calculation:**

- Total calls per dossier: 133
- Spread over: 5-10 minutes (300-600 seconds)
- Average rate: 133 / 450 seconds = **0.3 calls/second**
- Rate limit: 100 calls / 100 seconds = **1 call/second**

**Safety Factor:** 3.3Ã— under limit âœ…

**Concurrent Runs:**
- 80 dossiers on Monday
- If all start at once: 80 Ã— 0.3 = 24 calls/second
- **Would exceed limit!**

**Solution:**
- Stagger starts (10 per minute)
- Or: Use batch reads (3 reads â†’ 1 call)
- Or: Just accept some will queue/retry (Make.com handles this)

**Verdict:** Safe with staggered starts âœ…

---

## âœ… Supabase Migration Path

**Current (Google Sheets):**
```
[1] Google Sheets > Get Values (Clients!A2:M100)
    Filter: client_id = {{client_id}}
[2] LLM Call
[3] Google Sheets > Add Row (PipelineSteps)
```

**Future (Supabase):**
```
[1] HTTP > GET api.columnline.dev/clients/{{client_id}}
[2] LLM Call
[3] HTTP > POST api.columnline.dev/pipeline_steps
    Body: {step_id, run_id, input, output, ...}
```

**Changes Needed:**
- Replace Google Sheets modules with HTTP modules
- API endpoints match sheet structure (same fields)
- No scenario logic changes

**Verdict:** Migration path clear âœ…

---

## âœ… Documentation Complete

**Files Created:**

1. âœ… `tmp/google-sheets-schema-summary.md` - Complete schema design with Make.com architecture section
2. âœ… `tmp/MAKECOM_SHEETS_REFERENCE.md` - Google Sheets API reference for Make.com
3. âœ… `tmp/MAKECOM_API_CALLS_REFERENCE.md` - Complete breakdown of all API calls per scenario
4. âœ… `tmp/SCENARIO_ARCHITECTURE.md` - Isolated scenario pattern explained
5. âœ… `tmp/NEXT_STEPS_SHEETS_IMPORT.md` - Step-by-step import guide
6. âœ… `tmp/PROMPT_GAP_ANALYSIS.md` - Prompt status and gap analysis
7. âœ… `tmp/SCHEMA_VALIDATION_CHECKLIST.md` - This file

**CSV Files Ready:**

1-14. âœ… All 14 CSV files in `tmp/sheets_export/`
- Config layer with example data (Clients, Prompts, SectionDefinitions)
- Execution layer with empty templates (11 sheets)

---

## ðŸš€ Ready to Build Checklist

**Before You Start Tonight:**

- [ ] Import all 14 CSVs to Google Sheets (~15 min)
- [ ] Get Sheet ID from URL
- [ ] Add GOOGLE_SHEETS_ID variable to Make.com
- [ ] Test read Clients + Prompts
- [ ] Test write to Runs sheet

**Building First Scenario:**

- [ ] Pick one simple scenario (Enrich Lead recommended)
- [ ] Create test data manually (1 row in Runs, 1 row in Claims)
- [ ] Build scenario using API reference
- [ ] Test in isolation
- [ ] Verify PipelineSteps has input/output columns populated

**Expanding to Full Pipeline:**

- [ ] Clone tested scenario for other steps
- [ ] Build bridge scenarios (Enrich Contacts, Section Writers)
- [ ] Connect main pipeline to call all scenarios
- [ ] Test with 1 full dossier end-to-end
- [ ] Check all sheets populated correctly

---

## âœ… Final Validation

**Schema Design:**
- [x] Solves concurrent overwrite problem
- [x] Config separated from execution
- [x] Per-run isolation via run_id
- [x] Input/output denormalization for visibility
- [x] Simple, flexible, non-fragile (JSON blobs)

**Make.com Architecture:**
- [x] Isolated scenarios (pass IDs, fetch everything)
- [x] Each scenario independently testable
- [x] Clear data flow (read â†’ process â†’ write)
- [x] API calls manageable (~133 per dossier)

**Migration Path:**
- [x] Future-proof for Supabase
- [x] No scenario logic changes needed
- [x] Just swap modules (Sheets â†’ HTTP)

**Documentation:**
- [x] Complete schema reference
- [x] API call breakdown
- [x] Scenario patterns documented
- [x] Import guide ready

---

**Status:** âœ…âœ…âœ… **READY TO BUILD**

You have everything you need to start editing Make.com scenarios tonight. Good luck! ðŸš€
